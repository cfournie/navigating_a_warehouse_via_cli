<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      body { font-family: 'Roboto'; }
      h1, h2, h3 {
        font-family: 'Roboto';
        font-weight: bold;
        color: #FFFFFF;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .remark-slide-content {
        background-color: #223274;
        color: #FFFFFF;
        background-size: auto 100%;
      }
      .remark-slide-container {
      visibility: hidden;
          display: initial;
      }
      .remark-visible {
          visibility: visible;
      }
      em {
        color: #fb8b8a;
        font-style:normal;
      }
      .statistic {
        font-size: 85pt;
        font-weight: bold;
        color: #44bebf;
      }
      .footer {
        position: absolute;
        bottom: 12px;
        left: 20px;
        opacity: .5;
      }
    </style>

    <link rel="stylesheet" href="./mermaid.dark.css">
  </head>
  <body>
    <textarea id="source">

layout: true
class: center, middle


<div class="footer"><code>https://github.com/cfournie/navigating_a_warehouse_via_cli</code></div>
---

# Navigating a Data Warehouse via CLI

Chris Fournier (@cfournie)

<br/><br/>

<img src="./shopify_monotone_white.svg" width="120pt" />

???

Hi, I'm Chris Fournier and I work at Shopify.

In case you haven't heard of us...


---


background-image: url(./frontpage.png)
background-position: center;
background-repeat: no-repeat;

???

... we provide a platform for people to sell products online or in person.


---


## Data Warehouse

*Platform* to collect, store, analyze, and report on *data*

???

The specific team that I work on develops software for and maintains a data warehouse, or in other words a platform used to collect, store, analyze, and report on data outside of your web app or other operational systems.


Our anaylsts us it to answer questions about the performance of Shopify as a business, our products, and our features.


---


### GMV

<img src="./gmv.png" height="480pt" />

???

One example of a question that we try to answer for Shopify as a whole is:

"What is the total value of everything that has been sold through our platform?"

We call that gross merchandise value (or GMV) and that's one of the metrics that we use as a company to gauge our success.

This is not a trivial number to calculate given the volume of orders and transactions that we process and the analysis that we do to break this number down by payment gateway, industry, monthly recurring revenue, etc. We're also continuously processing data to keep these numbers up to date.

But from a broad techincal perspective, how do we make this dataset?


Sources:
https://www.slideshare.net/SolmazShahalizadeh/building-a-financial-data-warehouse-a-lesson-in-empathy
https://s2.q4cdn.com/024715446/files/doc_presentations/2017/11/Investor-Deck-Q3-2017.pdf

---
class: center, middle

## Batch jobs

### (E)xtract, (T)ransform, and (L)oad

<div class="mermaid">
graph LR
    A(Extract<br>orders)-- raw orders -->C(Transform into<br>GMV per shop)
    B(Extract<br>transactions)-- raw transactions -->C
    C-- GMV per shop -->D(Load to<br>a Database)
</div>

<br><br><br>


???

We predominantly run ETL batch jobs that:
  - Extract batches of data from operational systems (like MySQL) and stores it elsewhere (for example as a dataset called "raw orders")
  - Transform it into a dataset that we can use to answer questions about a specific metric (for example "GMV per shop"), and then
  - Load that dataset into a database for analysts to issue queries against and create reports upon

These batch jobs are written mostly in Python by our analysts (with the help of our engineers).  The number of these batch jobs, their implementations, and whether a job reads a specific dataset or not changes frequently. How frequently?

---

<h2>Development speed</h2>

.center[
<span style="float:left;">
<span class="statistic">1100+</span><br>
ETL job implementations<br>(as of Nov 1017)
</span>

<span style="float:right;">
<span class="statistic">15.2</span><br>
Deploys per day in 2017 on<br>average (Â±8.6 std dev)
</span>
]

<br><br><br><br><br><br><br><br><br><br><br>



???

We have over 1100+ job implementations, and so far in 2017 on average we deploy about 15 different versions of these jobs per day.

The engineering team that I work on not only maintains the platform that these jobs are developed and run on, but also helps our analysts keep these jobs running reliably.

To help make sure that these jobs are working, we need to be able to navigate this ever-changing set of jobs, their implementations, and the relationships between them to debug issues.

How do we do that?


---


background-image: url(./hue_workflow.png)
background-position: center;
background-repeat: no-repeat;


???

One way that we navigate our jobs is through a Django app called Hue. It's a very handy way to visualize groups of jobs (which we refer to as flows) and the jobs themselves.


---

background-image: url(./hue_logs.png)
background-position: center;
background-repeat: no-repeat;

???

It can even stream job logs for us.


---


background-image: url(./hue_workflows.png)
background-position: center;
background-repeat: no-repeat;

???

And can also show lists of jobs that are completed or being run by our scheduler, Oozie.

One downside to using Hue is that it was not meant for the number of jobs that we run; its UI renders sluggishly and we can only search by a flow's (or group of jobs) name, and not by any of our job names or metadata (like the where it reads/writes, which team maintains it, etc.).

So how can we get around that?

---

       
### Search YAML flow and job definitions

.left[
```yaml
order-transaction-states-incremental:
  owner: team@example.com
  frequency: 2h
  service-level-objective: 4h

  extract-orders:
    executable: jobs/extract_orders.py
    inputs: ['orders@database']
    output: /data/raw/orders/

  extract-transactions:
    executable: jobs/extract_transactions.py
    inputs: ['transactions@database']
    output: /data/raw/transactions/

  compute-gmv:
    resource_class: xxlarge
    executable: jobs/compute_gmv.py
    inputs: ['/data/raw/orders/', '/data/raw/transactions/']
    output: /data/frontroom/gmv-facts/

  load-gmv:
    resource_class: xxlarge
    executable: jobs/loader.py
    input: /data/frontroom/gmv-facts/
    output: shopify.gmv_facts@database
```
]

???

Well we store the definition of our schedule in a YAML file in our git repository, and the version that we have in our master branch is at most 20 minutes newer than what could be running in our production environment, so it's a pretty good analogue for production.

We can open the YAML files, look around, for example this is the set of GMV jobs (or GMV flow) that I showed you earlier: this group of jobs has a team that owns it, it runs every 2h, shouldn't be more than 4h behind, it has two extraction jobs, one job that reads the raw extracted data and outputs a dataset describing GMV, and another that loads that dataset into a database.

With this YAML we can use our editor's find tools (or grep) and with some regex we can get some aggregate statistics on what's in the file like how many jobs use "xxlarge" resources or how many jobs do we have in total.

But except for finding specific jobs details and some basic aggregate statistics, there's not a lot of analytical power here, and remember that we have over 1100 jobs. That's a lot, and when alayzing classes of failures, we can to be able to break down that list so that we can better understand which jobs are impacted by an issue. We need a table and the ability to filter by various job properties.

We can't get that from YAML. We can't easily break down our list of jobs by the resources that they use, their inputs or outputs, or even any of properties of the implementations themselves. They're implemented in Python using declarative APIs, if we instantiated those modules we could learn about even about how they're set up and what they do.

Thankfully, since we use this YAML file to specify out schedule, but we still need to interpret it and send changes to it to our scheduler, Oozie, we already have a set of Python APIs to parse and iterate over it. So we create this table using Python.

---


## Parse and print jobs

.left[
```python
flows = lib.generate_schedule()
for flow in flows.values():
    for name, job in flow.jobs.items():
        print(
            name,
            job.resource_class.value,
            job.executable,
            job.inputs,
            job.output
        )
```
]


???

So let's script this! To get some more analytical power, let's read this schedule and compose a table of all of our flows (to start) and just print the output.


---

## Solution 2

Output a table of jobs.
 
---

## Which jobs use xxlarge resources?

`awk`

---

## Just the names of xxlarge resource jobs

`cut`

---

## Names and implementations

`cut`, `column -t`

---

## Which jobs have no implementation?

`awk` for "NA"


---

## Denormalized data

Show flow and job data together

---


## Dataset graph

---


## Flatten as input, output

---

## Flatten as input, downstream output, depth

---

## Data poisioning uses

Which jobs are downstream of other jobs that are of type X?

List jobs of type X

List dowstream of all jobs

Need to join!

---

## Join

---

## Arbitrary Qs possible

---

## Speed

- Perception of speed (us generators and output each line as generated)
- Profile using cprofile
- Generate cols on demand
- Cache output (invalidate on input changes)

---

## Testing

- Move as much code into your models and unit test there
- Script should produce parseable output; mock its data and parse it
- Integration test against real data (answer a Q that should remain stable)

    </textarea>
    <link rel="stylesheet" href="tomorrow-night-blue.css">
    <script src="highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="remark.min.js"></script>

    <script src="./mermaid.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    
    <script>
      var slideshow = remark.create({
        // Set the slideshow display ratio
        // Default: '4:3'
        // Alternatives: '16:9', ...
        ratio: '4:3',

      highlightLanguage: 'python',
      highlightStyle: 'tomorrow-night-blue',
      highlightLines: true,
      
        // Navigation options
        navigation: {
          // Enable or disable navigating using scroll
          // Default: true
          // Alternatives: false
          scroll: true,

          // Enable or disable navigation using touch
          // Default: true
          // Alternatives: false
          touch: true,

          // Enable or disable navigation using click
          // Default: false
          // Alternatives: true
          click: false
        },

        // Customize slide number label, either using a format string..
        slideNumberFormat: 'Slide %current% of %total%',
        // .. or by using a format function
        slideNumberFormat: function (current, total) {
          return current + ' of ' + total;
        },

        // Enable or disable counting of incremental slides in the slide counting
        countIncrementalSlides: true
      }); 
    </script>
  </body>
</html>
