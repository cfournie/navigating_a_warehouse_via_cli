<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      body { font-family: 'Roboto'; }
      h1, h2, h3 {
        font-family: 'Roboto';
        font-weight: bold;
        color: #FFFFFF;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .remark-slide-content {
        background-color: #223274;
        color: #FFFFFF;
        background-size: auto 100%;
      }
      .remark-slide-container {
      visibility: hidden;
          display: initial;
      }
      .remark-visible {
          visibility: visible;
      }
      em {
        color: #fb8b8a;
        font-style:normal;
      }
      .statistic {
        font-size: 85pt;
        font-weight: bold;
        color: #44bebf;
      }
      .footer {
        position: absolute;
        bottom: 12px;
        left: 20px;
        opacity: .5;
      }
    </style>

    <link rel="stylesheet" href="./mermaid.dark.css">
  </head>
  <body>
    <textarea id="source">

layout: true
class: center, middle


<div class="footer"><code>https://github.com/cfournie/navigating_a_warehouse_via_cli</code></div>


---


# Navigating a Data Warehouse via CLI

Chris Fournier (@cfournie)

<br/><br/>

<img src="./shopify_monotone_white.svg" width="120pt" />

???

Hi, I'm Chris Fournier and I work at Shopify.

In case you haven't heard of us...


---


background-image: url(./frontpage.png)
background-position: center;
background-repeat: no-repeat;

???

... we provide a platform for people to sell products online or in person.


---


## Data Warehouse

*Platform* to collect, store, analyze, and report on *data*

???

The specific team that I work on develops software for and maintains a data warehouse, or in other words a platform used to collect, store, analyze, and report on data outside of your web app or other operational systems.


Our anaylsts us it to answer questions about the performance of Shopify as a business, our products, and our features.


---


### GMV

<img src="./gmv.png" height="480pt" />

???

One example of a question that we try to answer for Shopify as a whole is:

"What is the total value of everything that has been sold through our platform?"

We call that gross merchandise value (or GMV) and that's one of the metrics that we use as a company to gauge our success.

This is not a trivial number to calculate given the volume of orders and transactions that we process and the analysis that we do to break this number down by payment gateway, industry, monthly recurring revenue, etc. We're also continuously processing data to keep these numbers up to date.

But from a broad techincal perspective, how do we make this dataset?


Sources:
https://www.slideshare.net/SolmazShahalizadeh/building-a-financial-data-warehouse-a-lesson-in-empathy
https://s2.q4cdn.com/024715446/files/doc_presentations/2017/11/Investor-Deck-Q3-2017.pdf


---
class: center, middle


## Batch jobs

### (E)xtract, (T)ransform, and (L)oad

<div class="mermaid">
graph LR
    A(Extract<br>orders)-- raw orders -->C(Transform into<br>GMV per shop)
    B(Extract<br>transactions)-- raw transactions -->C
    C-- GMV per shop -->D(Load to<br>a Database)
</div>

<br><br><br>


???

We predominantly run ETL batch jobs that:
  - Extract batches of data from operational systems (like MySQL) and stores it elsewhere (for example as a dataset called "raw orders")
  - Transform it into a dataset that we can use to answer questions about a specific metric (for example "GMV per shop"), and then
  - Load that dataset into a database for analysts to issue queries against and create reports upon

These batch jobs are written mostly in Python by our analysts (with the help of our engineers).  The number of these batch jobs, their implementations, and whether a job reads a specific dataset or not changes frequently. How frequently?


---


## Development speed

.center[
<span style="float:left;">
<span class="statistic">1100+</span><br>
ETL job implementations<br>(as of Nov 1017)
</span>

<span style="float:right;">
<span class="statistic">15.2</span><br>
Deploys per day in 2017 on<br>average (Â±8.6 std dev)
</span>
]

<br><br><br><br><br><br><br><br><br><br><br>



???

We have over 1100+ job implementations, and so far in 2017 on average we deploy about 15 different versions of these jobs per day.

The engineering team that I work on not only maintains the platform that these jobs are developed and run on, but also helps our analysts keep these jobs running reliably.

To help make sure that these jobs are working, we need to be able to navigate this ever-changing set of jobs, their implementations, and the relationships between them to debug issues.

How do we do that?


---


background-image: url(./hue_workflow.png)
background-position: center;
background-repeat: no-repeat;


???

One way that we navigate our jobs is through a Django app called Hue. This isn't the latest version of Hue here, but the UI is essentially the same. It's a very handy way to visualize groups of jobs (which we refer to as flows) and the jobs themselves.


---


background-image: url(./hue_workflows.png)
background-position: center;
background-repeat: no-repeat;

???

Hue can also show lists of jobs that are completed or being run by our scheduler, Oozie.

One downside to using Hue is that it was not meant for the number of jobs that we run; its UI renders sluggishly and we can only search by a flow's (or group of jobs) name, and not by any of our job names or metadata (like the where it reads/writes, which team maintains it, etc.). This is an obstacle when we're trying to explore our jobs and understand how they're structured.

So how can we gain a bit more speed and access to more metadata to search through?


---

       
### Search YAML flow and job definitions

.left[
```yaml
order-transaction-states-incremental:
  frequency: 2h

  extract-orders:
    executable: jobs/extract_orders.py
    inputs: ['orders@database']
    output: /data/raw/orders/

  ...

  compute-gmv:
    resource_class: xxlarge
    executable: jobs/compute_gmv.py
    inputs: ['/data/raw/orders/', '/data/raw/transactions/']
    output: /data/frontroom/gmv-facts/

  load-gmv:
    resource_class: xxlarge
    executable: jobs/loader.py
    inputs: /data/frontroom/gmv-facts/
    output: shopify.gmv_facts@database
```
]

???

Well we store the definition of our schedule in a YAML file in our git repository, and the version that we have in our master branch is at most 20 minutes newer than what could be running in our production environment, so it's a pretty good analogue for production.

We can open the YAML files, look around, for example this is the set of GMV jobs (or the GMV flow) that I showed you earlier: this group of jobs has a team that owns it, it runs every 2h, shouldn't be more than 4h behind, it has an extraction job that produces raw dara, one job that reads that raw data and outputs a dataset describing GMV, and another that loads that GMV dataset into a database.

With this YAML we can use our editor's find tools (or grep) and with some regex we can get some aggregate statistics on our schedule, like how many jobs use "xxlarge" resources or how many jobs do we have in total.

But except for finding details for specific jobs and some basic aggregate statistics, there's not a lot of analytical power here, and remember that we have over 1100 jobs. That's a lot, and when alayzing classes of failures, we'd like to be able to break down that list by various job metadata so that we can better understand which jobs are impacted by an issue.


---


### Format YAML as a table

.left[
Job                     | Resources &nbsp;   | Executable | Etc.
----------------------- | -------------- | -----------
cuddly-two              | large    | jobs/cuddly-two.py | ...
load-cuddly-two         | xxlarge  | jobs/load-cuddly-two.py
untidy-enthusiasm       | small    | jobs/untidy-enthusiasm.py
load-untidy-enthusiasm  | large    | jobs/load-untidy-enthusiasm.py
first-restaurant        | small    | jobs/first-restaurant.py
load-first-restaurant   | small    | jobs/load-first-restaurant.py
jazzy-comfortable       | xxlarge  | jobs/jazzy-comfortable.py
load-jazzy-comfortable  | medium   | jobs/load-jazzy-comfortable.py
annoyed-morning         | xxlarge  | jobs/annoyed-morning.py
ubiquitous-emphasis     | large    | jobs/ubiquitous-emphasis.py
]

<br><br><br>


???

We this YAML data layed out in  a table, and we need the ability to filter by columns.

We can't get that directly from the YAML itself. We also can't get any of properties of the implementations themselves. They're implemented in Python using declarative APIs, and if we instantiated those modules we could learn even more about how they're set up and what they do.

So we can't get this data directly from the YAML, but thankfully we already have libraries that parse this YAML to update our scheduler, Oozie. We have APIs that convert this YAML into XML definitions and Oozie API calls, and we can reuse those APIs and create this table using a Python script!


---


### Parse and print jobs

.left[

```python
flows = lib.generate_schedule()

for flow in flows.values():
    for name, job in flow.jobs.items():
        print(
            name,
            job.resource_class.value,
            job.executable,
            job.inputs,
            job.output
        )
```
]


???

So let's script this! Let's keep it simple and just read this schedule and to compose a table let's just print eveything we know about each job on one line.

What does this look like?


---


`python scripts/jobs1.py`

???

Garbage.


---



### *Whitespace* was a mess.

### Jobs have a *variable* number of inputs.



???

What went wrong?

Well there was a lot of output.

Whitespace was a mess, we couldn't tell where the columns were.

Jobs also have multiple inputs, so we're basically cramming an array of inputs into one cell in this table. Not great.

So let's try something similar, but this time let's omit inputs and I'll show you one of my favourite commands to help format output on the CLI.


---



RUN python scripts/jobs2.py | column -t

 
---


### `python scripts/jobs2.py | column -t`

???

What did I just do there? Two things:

1. I removed our variable input column
2. I piped the output the Python script to another program called "column" and asked it to format the output into a table

What what is piping magic you might ask?

This is some of the magic that Linux/BSD/OSX can provide you so that you can combine your simple script with many other UNIX tools to create some truly fantastic output.


---


### `python scripts/jobs2.py | column -t`
<br>
.left[
### 1. Call *`pipe(2)`* to create a file (e.g. `pipe:a`)
### 2. *Fork* twice (for `python` and `column`)
### 3. Set `python`'s *`stdout`* to *`pipe:a` write* end
### 4. Set `column`'s *`stdin`* to *`pipe:a` read* end
### 5. Run `python` & `column` roughly *in parallel*
]

???

Roughly how this works is:
1. Bash will create a special in-memory pipe file in the pipe filesystem
2. Bash will then fork twice to create two child processes, one for python and one for column
3. Bash will set python's stdout (a file descriptor where it writes output to during print) to the write end of the pipe file
4. Bash will set column's stdin (a file descriptor where it reads input from) to the read end of that same pipe file
5. Bash then starts both processes in parrallel and they write and read from the pipe file until they're finished.

So we can make a super simple Python script and let bash and other unix programs massage our output into our desired form, and we get to spread this work over multiple cores.

That's awesome.

Let's take this further!


---


RUN python scripts/jobs3.py | cut -f1 -f2 | column -t

???
Let's change our python program a little and output tab separators instead of spaces. Why?
Because a lot of the unix tools assume that fields i.e. columns are tab-separated.

Once we do that, let's use a program called `cut` AND `column` to show a table of just job names and their resource class.

`cut` can be used to say "give me only the first and second field (or column) in each row, and then we can pass just those two columns to the `column` program.


---


.left[
```
RUN python scripts/jobs3.py | wc -l
RUN python scripts/jobs3.py | grep large | column -t
RUN python scripts/jobs3.py | awk '$2 == "large"' | column -t
RUN python scripts/jobs3.py | awk '$2 == "large"' | wc -l
RUN python scripts/jobs3.py | head
```
]

???
Let's take this further and flex some more of our rumoured analytical power here.

How do we answer questions like:

"How many jobs do we have? For that we can pipe our output to the `wc` or word count program and ask it to count lines"

A little more advanced:

"Which jobs use the large resource class?" Grep is good for filtering, but awk is even better at specifying exact values to compare a column to

"How many large jobs are there?"

Now all of those calls produced a lot of output, which is difficult to read when your fiddling with these programs and trying to figure out which parameters to use. One way to make prototyping these commands easier is to simply chop that output down to just a few lines or so using `head`. Let's try that.

Uh oh!  There's a garbled stack trace in there. What happened?


---


### `python scripts/jobs3.py | head`

.left[
```
Traceback (most recent call last):
  File "scripts/jobs3.py", line 13, in <module>
    job.output
BrokenPipeError: [Errno 32] Broken pipe
```
]

???

Getting rind of some of the output we can see that what we got was a `BrokenPipeError`.

That happened because `head` does something special. Head is reading from the pipe file all the output from the python script up until it has decided that it has read enough, in this case by default after it sees 10 lines.

Once it sees 10 lines, it closes the read end of the pipe file.

But our Python script doesn't know that `head` has had enough and it still wants to write more output to the write end of the pipe file.

When the Python script tries to write its 11th line of output, we get back from the OS an `EPIPE` errors which Python wraps for us as a `BrokenPipeError`.

Thankfully, the OS provides us a way to tell that we should stop writing.


---


### `python scripts/jobs4.py | head`

.left[
```python
import signal

def handle_sigpipe(signum, frame):
    sys.stderr.close()
    sys.exit(0)

signal.signal(signal.SIGPIPE, handle_sigpipe)
```
]

???

When all file descriptors pointing to the read end of the pipe have been closed, our Python script is sent a signal called SIGPIPE, indicating that the pipe that we were writing to is closed and we should shut down.

All we need to do in our script is write and register a signal handler which will suppress any more stderr output (which is a different output file descriptor where errors are sent) and simply end the program when we receive the SIGPIPE signal.


---


RUN python scripts/jobs4.py | head | column -t

???

Now we're exception free, let's take a look at some more handy unix tools and the problems that they can solve.


---


.left[
```
RUN python scripts/jobs4.py | cut -f1 -f2 -f3 | sort -n3 | head  # TODO requires flow freq
RUN python scripts/jobs4.py | cut -f5 | sort | uniq
```
]

???

What are the most frequently-run jobs?
Which resource classes are used?


---


Job graph


---


Downstream grain


---


All datasets downstream of a job that uses small resources?

Filter jobs that use small resources and produce output
From output, we could get all downstream

Need join


---


Join!


---


Talk about data poisioning and the need for beskope queries.


---


.left[
###- *`head`* to truncate output for prototyping
###- *`column -t`* to pretty-print a table
###- *`grep`* and *`awk`* to filter rows
###- *`wc -l`* to count (filtered) rows
###- *`sort`* to sort output
###- *`uniq`* to filter out duplicate rows
###- *`join`* to join two sorted files on a column
]


---


## Testing

.left[
- Move as much code into your models and unit test there
- Script should produce parseable output; mock its data and parse it
- Integration test against real data (answer a Q that should remain stable)
]


---


## Performance

.left[
- cProfile
- generators for head
- pandas for full output
- use options to enable/disable rendering expensive cols
- cache output (e.g. joblib.memory)
]


---


## References

.left[
- `https://www.slideshare.net/shopifyInvestors/investor-deck-q3-2017-81481241`
- `https://www.slideshare.net/SolmazShahalizadeh/building-a-financial-data-warehouse-a-lesson-in-empathy`
- `https://brandonwamboldt.ca/how-linux-pipes-work-under-the-hood-1518/`
]

## Thanks

Karl Taylor, Aaron Olsen, and the<br>Data Acceleration team @ Shopify


    </textarea>
    <link rel="stylesheet" href="tomorrow-night-blue.css">
    <script src="highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="remark.min.js"></script>

    <script src="./mermaid.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    
    <script>
      var slideshow = remark.create({
        // Set the slideshow display ratio
        // Default: '4:3'
        // Alternatives: '16:9', ...
        ratio: '4:3',

      highlightLanguage: 'python',
      highlightStyle: 'tomorrow-night-blue',
      highlightLines: true,
      
        // Navigation options
        navigation: {
          // Enable or disable navigating using scroll
          // Default: true
          // Alternatives: false
          scroll: true,

          // Enable or disable navigation using touch
          // Default: true
          // Alternatives: false
          touch: true,

          // Enable or disable navigation using click
          // Default: false
          // Alternatives: true
          click: false
        },

        // Customize slide number label, either using a format string..
        slideNumberFormat: 'Slide %current% of %total%',
        // .. or by using a format function
        slideNumberFormat: function (current, total) {
          return current + ' of ' + total;
        },

        // Enable or disable counting of incremental slides in the slide counting
        countIncrementalSlides: true
      }); 
    </script>
  </body>
</html>
