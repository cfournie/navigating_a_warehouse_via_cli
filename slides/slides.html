<!DOCTYPE html>
<html>
  <head>
    <title>Title</title>
    <meta charset="utf-8">
    <style>
      body { font-family: 'Roboto'; }
      h1, h2, h3 {
        font-family: 'Roboto';
        font-weight: bold;
        color: #FFFFFF;
      }
      .remark-code, .remark-inline-code { font-family: 'Ubuntu Mono'; }
      .remark-slide-content {
        background-color: #223274;
        color: #FFFFFF;
        background-size: auto 100%;
      }
      .remark-slide-container {
      visibility: hidden;
          display: initial;
      }
      .remark-visible {
          visibility: visible;
      }
      em {
        color: #fb8b8a;
        font-style:normal;
      }
      .statistic {
        font-size: 85pt;
        font-weight: bold;
        color: #44bebf;
      }
      .footer {
        position: absolute;
        bottom: 12px;
        left: 20px;
        opacity: .5;
      }
      td, th {
        padding: 0 6pt;
      }
    </style>

    <link rel="stylesheet" href="./mermaid.dark.css">
  </head>
  <body>
    <textarea id="source">

layout: true
class: center, middle


<div class="footer"><code>https://github.com/cfournie/navigating_a_warehouse_via_cli</code></div>


---


# Navigating a Data Warehouse via CLI

<br>

Chris Fournier (@cfournie)

<br/><br/>

<img src="./shopify_monotone_white.svg" width="150pt" />

???

Hi, I'm Chris Fournier, I work at Shopify, and I'm going to talk to you today about a simple tool that I wrote in Python to navigate our data warehouse and how to leverage other unix tools to get the most of such scripts.

In case you haven't heard of Shopify...


---


background-image: url(./frontpage.png)
background-position: center;
background-repeat: no-repeat;

???

... we provide a platform for people to sell products online or in person.


---


## Data Warehouse

*Platform* to collect, store, analyze, and report on *data*

???

The specific team that I work on develops software for and maintains a data warehouse, or a platform used to collect, store, analyze, and report on data collected from our systems.

Our analysts use it to answer questions about the performance of Shopify as a business, our products, and our features.


---


### Gross Merchandise Volume (GMV)

<img src="./gmv.png" height="480pt" />

???

One example of a question that we try to answer for Shopify as a whole is:

"What is the total value of everything sold through our platform?"

Or Gross Merchandise Volume (GMV).

That's one of the metrics that we use as a company to gauge our success.

This is not a trivial number to calculate given the volume of orders and transactions that we process and the logic that we use to break this number down by payment gateway, industry, monthly recurring revenue, etc. In 2016 alone we had over $15 billion dollars worth of GMV and that's a lot of rows. We're also continuously processing data to keep these numbers up to date.

But from a broad techincal perspective, how do we make this dataset and keep it up to date?


Sources:
https://www.slideshare.net/SolmazShahalizadeh/building-a-financial-data-warehouse-a-lesson-in-empathy
https://s2.q4cdn.com/024715446/files/doc_presentations/2017/11/Investor-Deck-Q3-2017.pdf


---
class: center, middle


## Batch jobs

### (E)xtract, (T)ransform, and (L)oad

<div class="mermaid">
graph LR
    A(Extract<br>Orders)-- Raw Orders -->C(Transform into<br>GMV per Shop)
    B(Extract<br>Transactions)-- Raw Transactions -->C
    C-- GMV per Shop -->D(Load to<br>a Database)
</div>

<br><br><br>


???

We predominantly run ETL batch jobs every few hours that:
  - Extract batches of data from operational systems (like MySQL) and store it elsewhere (for example as a dataset called "raw orders")
  - Transform it into a dataset that we can use to answer questions about a specific metric (for example "GMV per shop"), and then
  - Load that dataset into a database for analysts to issue queries against and create reports upon

The batch jobs that we run in our warehouse are written mostly in Python by our analysts (with the help of our engineers) and the number of these batch jobs, their implementations, and whether a job reads a specific dataset or not changes frequently. How frequently?


---


## Development speed

.center[
<span style="float:left;">
<span class="statistic">1100+</span><br>
ETL job implementations<br>(as of Nov 1017)
</span>

<span style="float:right;">
<span class="statistic">15.2</span><br>
Deploys per day in 2017 on<br>average (Â±8.6 std dev)
</span>
]

<br><br><br><br><br><br><br><br><br><br><br>



???

We have over 1100+ job implementations, and so far in 2017 on average we deploy about 15 different versions of these jobs per day.

The engineering team that I work on not only maintains the platform and frameworks that these jobs are developed and run on, but also helps our analysts keep these jobs running reliably.

To help make sure that these jobs are working, we need to be able to navigate this ever-changing set of jobs, their implementations, and the relationships between them to debug issues.

How do we do that?


---


background-image: url(./hue_workflow.png)
background-position: center;
background-repeat: no-repeat;


???

One way that we navigate our jobs is through a Django app called Hue. This isn't the latest version of Hue here, but the UI is essentially the same. It's a very handy way to visualize groups of jobs (which we refer to as flows) and the jobs themselves.


---


background-image: url(./hue_workflows.png)
background-position: center;
background-repeat: no-repeat;

???

Hue can also show lists of jobs that are completed or being run by our scheduler, Oozie.

One downside to using Hue is that it was not meant for the number of jobs that we run; its UI renders sluggishly and we can only search by a flow's (or group of jobs) name, and not by any of our job names or other metadata (like the where it reads/writes, which team maintains it, etc.). This is an obstacle when we're trying to explore our jobs and understand how they're structured or who to talk to to help us better understamd what they do.

So how can we gain a bit more speed and access to more metadata to search through?


---

       
### Search YAML flow and job definitions


<div style="margin: auto;width: 80%">
.left[
```yaml
order-transaction-states-incremental:
  frequency: 2h

extract-orders:
    resource_class: medium
    executable: jobs/extract_orders.py
    inputs: ['orders@database']
    output: /data/raw/orders/

  ...

  compute-gmv:
    resource_class: xxlarge
    executable: jobs/compute_gmv.py
    inputs: ['/data/raw/orders/', '/data/raw/transactions/']
    output: /data/frontroom/gmv-facts/

  load-gmv:
    resource_class: xxlarge
    executable: jobs/loader.py
    inputs: /data/frontroom/gmv-facts/
    output: shopify.gmv_facts@database
```
]
</div>

???

All of our jobs are scheduled to run periodically by our scheduler Oozie, but we store the definition of our schedule (and a lot of metadata) in a YAML file in our git repository alongside our batch job implementations. We can just search this file to learn more about our jobs.

We can open the YAML files, look around, for example this is the set of GMV jobs (or the GMV flow) that I showed you earlier: this has been simplified for brevity but we can see that it runs all of these jobs every 2h, it has an extraction job that produces raw data, one job that reads that raw data and outputs a dataset describing GMV, and another that loads that GMV dataset into a database.  We can also see the executable for each job, which datasets it reads/writes, and how many resources it uses (small, large, xxlarge, etc.). It's easy to find details about specific jobs.

With this YAML we can use our editor's find tools (or grep) and with some regex we can get some aggregate statistics on our schedule, like how many jobs use "xxlarge" resources or how many jobs do we have in total.

But except for finding details for specific jobs and some basic aggregate statistics, there's not a lot of analytical power here, and remember that we have over 1100 jobs that consume each others outputs. That's a lot of jobs and a lot of relationships between them, and when analyzing classes of failures, we'd like to be able to break down our jobs by their properties so that we can better understand which jobs are impacted by an issue. Maybe maybe jobs that are scheduled frequently and that use a "small" amount of resources are having an issue. Finding them is possible by looking at this YAML file, but not easy.


---


### Tables of metadata

.left[
Workflow (or group)   | Freq. (hours) | Job     | Resources | Etc.
--------------------- | ------------- | ------- | --------- | ---
abiding-poetry        |6  |complex-a            |small    | ...
abiding-poetry        |6  |defeated-obligation  |medium   |
abiding-poetry        |6  |kind-big             |xxlarge  |
abiding-poetry        |6  |load-complex-a       |small    |
abiding-poetry        |6  |load-kind-big        |xlarge   |
abiding-poetry        |6  |swift-energy         |medium   |
confused-independent  |7  |eight-studio         |small    |
confused-independent  |7  |load-eight-studio    |small    |
confused-independent  |7  |load-two-original    |small    |
confused-independent  |7  |luxuriant-medium     |xlarge   |
]

<br><br><br>


???

To more easily filter jobs by frequency and resources we need the data in our YAML files laid out as a table, and we need the ability to filter by columns.

We can't get a table directly from the YAML itself. We also can't get any of properties of the job implementations themselves unless we instantiate them and poke around how they're set up.

Thankfully we already have libraries that parse this YAML to update our scheduler. We have APIs that convert this YAML into XML definitions and Oozie API calls, and we can reuse those APIs and create the table that we want using a Python script!


---


### Parse and print metadata

<div style="margin: auto;width: 60%">
.left[
```python
for flow in flows:
    for job in flow.jobs:
        print(
            flow.name,
            flow.frequency,
            job.name,
            job.resource_class.value,
            job.executable,
            job.inputs,
            job.output
        )
```
]
</div>


???

So let's script this! Let's keep it simple and just read this schedule and to compose a table let's just print one job per line.

So let's iterate over each flow, or group of jobs, and then each job within and print the flow and job information.

Note that I'm printing the flow name and frequency repeatedly, once for each of the flow's jobs. This is repeticious, but this is intentional. Usually it would be bad to design a denormalized table like this, but this it will make filtering by flow properties that jobs inherit easier. Since we primarily care about jobs but we still want query jobs by flow properties, it makes sense to pre-join this table.  Denormalized tables are your friend on the CLI.

I'm sure some of you are a bit dubious of that claim, some DBAs are shaking their heads or are gettting up to leave, and the rest of you are wondering whether we can join tables in bash at all or whether all of this talk of tables is just an awful abuse of SQL as a metaphor. Well, stay tuned to find out!

So, what does this table look like?


---


`python scripts/jobs1.py`

???

Garbage.


---



### *Whitespace* was a mess.

### Jobs have a *variable* number of inputs.



???

What went wrong?

Well there was a lot of output.

Whitespace was a mess, we couldn't tell where the columns were.

Jobs also have multiple inputs, so we're basically cramming arrays of inputs into one column in this table. Not great. It's almost like having a variable number of columns.

So let's try something similar, but this time let's omit inputs and I'll show you one of my favourite commands to help format output on the CLI: column.


---



RUN python scripts/jobs2.py | column -t

 
---


### `python scripts/jobs2.py | column -t`

???

What did I just do there? Two things:

1. I removed our variable input column
2. I piped the output of the Python script to another program called "column" and asked it to format the output into a table

What is piping wizardry you might ask?

This is some of the magic that Linux/BSD/OSX can provide you so that you can combine your simple script with many other UNIX tools to create some truly fantastic output.


---


### `python scripts/jobs2.py | column -t`
<br>

<div class="mermaid">
graph LR
    A[<em><b>python scripts/jobs2.py</b></em><br>stdin=stdin<br>stdout=pipe file]-- write -->C(pipe file)
    C-- read-->D[<em><b>column -t</b></em><br>stdin=pipefile<br>stdout=stdout]
</div>

???

Here's the command that I just ran. I ran my script and its output was piped to the column program.

Roughly how this works is:
1. Bash will create a special in-memory pipe file in the pipe filesystem
2. Bash will then fork twice to create two child processes, one for python and one for column
3. Bash will set python's stdout (a file descriptor where it writes output to during print) to the write end of the pipe file
4. Bash will set column's stdin (a file descriptor where it reads input from) to the read end of that same pipe file
5. Both processes run roughly in parrallel and they write and read from the pipe file until they're finished.

So we can make a super simple Python script and let bash and other unix programs massage our output into our desired form, and we get to spread this work over multiple concurrently executing processes.

Now we're working with multiple cores and we don't need to handle threading ourselves. That's awesome.

Let's take this further!


---


RUN python scripts/jobs3.py | cut -f1 -f3 -f4 | column -t

???
Let's change our python program a little and output tab separators instead of spaces. Why?
Because a lot of the unix tools assume that fields i.e. columns are tab-separated.

Once we do that, let's use a program called `cut` AND the `column` program to show a table of just flow andjob names and the resource class.

`cut` can be used to say "give me only the first, third, and fourth field (or column) in each row, and then we can pass just those three columns to the `column` program.

What a tidy readable table!

And we accomplished this by not just piping to one program, but two. We can use pipes to chain as many programs together as we want.


---


.left[
```
RUN python scripts/jobs3.py | wc -l
RUN python scripts/jobs3.py | grep large | column -t
RUN python scripts/jobs3.py | awk '$4 == "large"' | column -t
RUN python scripts/jobs3.py | head
```
]

???
Let's take this further and flex some more of our rumoured analytical power here.

How do we answer questions like:

"How many jobs do we have? For that we can pipe our output to the `wc` or word count program and ask it to count lines"

A little more advanced:

"Which jobs use the large resource class?" Grep is good for filtering, but notice how in our search for "large" we also included "xlarge" and "xxlarge"? Oops. `awk` is a tool that can address that by specifying exact values to compare to specific columns.

Now all of those calls (except wordcount) produced a lot of output, which is difficult to read when your fiddling with these programs and trying to figure out which parameters to use. One way to make prototyping these commands easier is to simply chop that output down to just a few lines or so using `head`. Let's try that.

Uh oh!  There's a garbled stack trace in there. What happened?


---


### `python scripts/jobs3.py | head`

<div style="margin: auto;width: 70%">
.left[
```
Traceback (most recent call last):
  File "scripts/jobs3.py", line 13, in <module>
    job.output
BrokenPipeError: [Errno 32] Broken pipe
```
]
</div>

???

Getting rind of some of the output we can see that what we got was a `BrokenPipeError`.

That happened because `head` does something special. Head is reading from the pipe file all the output from the python script up until it has decided that it has read enough, in this case by default after it sees 10 lines.

Once it sees 10 lines, it closes the read end of the pipe file.

But our Python script doesn't know that `head` has had enough and it still wants to write more output to the write end of the pipe file.

When the Python script tries to write its 11th line of output, we get back from the OS an `EPIPE` errors which Python wraps for us as a `BrokenPipeError`.

Thankfully, the OS provides us a way to tell that we should stop writing.


---


### `python scripts/jobs4.py | head`

<div style="margin: auto;width: 70%">
.left[
```python
import signal

def handle_sigpipe(signum, frame):
    sys.stderr.close()
    sys.exit(0)

signal.signal(signal.SIGPIPE, handle_sigpipe)
```
]
</div>

???

When all file descriptors pointing to the read end of the pipe have been closed, our Python script is sent a signal called SIGPIPE, indicating that the pipe that we were writing to is closed and we should shut down.

All we need to do in our script is write and register a signal handler which will suppress any more stderr output (which is a different output file descriptor where errors are sent) and simply end the program when we receive the SIGPIPE signal.


---


RUN python scripts/jobs4.py | head | column -t

???

Now we're exception free, let's take a look at some more handy unix tools and the problems that they can solve.


---


.left[
```
RUN python scripts/jobs4.py | python scripts/jobs4.py | cut -f1 -f2 -f3 | sort -k2n | head
RUN python scripts/jobs4.py | cut -f4 | sort | uniq
```
]

???

There's a `sort` utility to sort by column which can even interpret text as numbers which we can use to answer:

"What are the most frequently-run jobs?"

There's also a `uniq` utility that can take a sorted output and return just the uniq values which we can use to answer:

"Which resource classes are used?"

Both useful utilities on their own, but where they really shine is in an application that I'll show you next.


---


### Dataset dependencies

<div class="mermaid">
graph LR
    A(Raw Orders)-->C(GMV per Shop)
    B(Raw Transactions)-->C
    B-->D(Transactions per Shop)
    D-->E(Transactions per Shop<br>and Gateway)
</div>

<br>

<div style="margin: auto;width: 80%">
.left[
Start                 | End
--------------------- | --------------
Raw Orders            | GMV per Shop
Raw Transactions      | GMV per Shop
Raw Transactions      | Transactions per Shop
Raw Transactions      | Transactions per Shop and Gateway
Transactions per Shop | Transactions per Shop and Gateway
]
</div>

<br><br><br>

???

Before I delve into that application though, I want to talk about another type of table that we can generate.

Previosly, I showed you a graph of batch jobs and the datasets that they produce. But we can also take the inputs and outputs of those jobs and compose a graph with datasets as nodes and incoming directed edges showing all of the datasets that are used to create that dataset.

For example, "GMV per Shop" is comprised of "Raw Orders" and "Raw Transactions".

This kind of a graph is interesting because we can use it to assess the impact that a mistake in producing one of our datasets would have on our other datasets. For example if we accidentally only extract some of our "Raw Orders" and not all of the, then only one other dataset, "GMV per Shop", will also be missing orders.

But if instead the "Raw Transactions" dataset is missing some transactions, then 3 other datasets will be missing data, first "GMV per Shop" and "Transactions per Shop" and then transitively "Transactions per Shop and Gateway" would also be missing transactions.

We can represent this in a tabular format with one row per path through this graph with the start and end of the path each being columns.

For example we can see that there is a path from:
- "Raw Transactions" to "GMV per Shop"
- "Raw Transactions" to "Transactions per Shop" and
- "Raw Transactions" to "Transactions per Shop and Gateway"


---


<div style="margin: auto;width: 80%">
.left[
```shell
python scripts/downstream.py | awk '$1 == "dataset"'
```
]
</div>

???

We can then use `awk` to ask:

"Which datasets are downstream of X?"

We could have also added columns so that we can see which team owns each dataset and the name of the producing job so that we can take actions on this information, like:

- Warn the owning teams of these datasets about missing or erroneous rows;
- Rebuilding these datasets in topological order to fix missing or erroneous rows; or
- Stop downstream jobs from running before they pick up bad rows.

Now let's imagine a more alarming scenario than just one bad dataset posioning downstream datasets.


---


<div style="margin: auto;width: 90%">
.left[

```shell
python scripts/jobs4.py | awk '$2 < 8 && $4 == "small"' > bad_jobs
python scripts/downstream.py > all_downstream
```
]
</div>

???

Let's say that we'd identified that all jobs that have a "small" resource class and a frequency less than 3 hours have been dropping rows. Maybe we accidentaly added a bug in our software where when we don't have a lot of memory and we run frequently enough we drop rows.

We can generate a list of jobs that meet those criteria and the datasets that they are poisoning.

We can also generate a list of everything downstream from something.

If we want to find out which datasets are affected, what we really need is the ability to join these two lists together on the job output and downstream start columns, and because there'd be duplicates, we'd want to `uniq` the output.

Can we do that via the CLI?


---


<div style="margin: auto;width: 40%">
<div style="float: left;width: 40%">
.left[
```
a
b
c
```
]
</div>

<div style="float: right;width: 40%">
.left[
```
a	1
b	2
b	3
d	4
```
]
</div>
</div>

<br><br><br><br><br>

<div style="margin: auto;width: 50%">
.left[
```shell
join -1 1 -2 1 file_a file_b
```
]
</div>

<br>


<div style="margin: auto;width: 20%">
.left[
```
a 1
b 2
b 3
```
]
</div>



???

Yes, yes we can :D

We can use the `join` command to perform an equality join between two sorted files.  It produces one row for each pair of lines that match in the two files.

For example, we can take these two files and join them on the first column of each file to produce the result below.

If we replace these letters with bad datasets in the first file and a dataset and a dataset downstream from it in the second file, we can join them together to produce a list of all datasets downstream of a bad dataset.


---


<div style="margin: auto;width: 100%">
.left[

```shell
cat bad_jobs | sort -k5 > sorted_bad_jobs
cat all_downstream | sort -k1 > sorted_all_downstream
join -1 5 -2 1 -t $'\t' sorted_bad_jobs sorted_all_downstream > joined
cat joined | cut -f6 | sort | uniq
```
]
</div>

???


First, we need to sort the data by the columns that we want to join on. For bar jobs that's the 5th column, with the job output. For all downstream that's the 1st column, the start of each path.

Once sorted, we can use the `join` command, say that we want to join on bad jobs column 5 and downstream column 1 (and separate by tabs again in the end) and we can save that to a file called joined.

Looking at that file we can see that it added the ends of each path to the last column here

From that file, we can then cut eveything except the 


---


### Script your own tables

### Filter with *`awk`* and use *`join`*


???


What I showed you here was an artificial scenario using some very simple scripts to make tables, but all of the principles are the same in the version that we use day to day.

Using awk to do filtering and then performing joins is essential for us because each of our failures that we've experienced has been unique, for example, we've had to search for all datasets downstream of jobs that:
- Incorrectly read only Parquet files when using filters
- Failed to read data from two newly added MySQL shards
- Read duplicates from a Kafka topic because Kafka was errioneiosly configured to mirror to itself
- Missed late-arriving data caused by a Kafka partition being unavailable

In all of these situations, being able to come up with an expression to filter our table of jobs and then join against the downstream table saved us countless minutes or even hours mainly because we prepared all of the metadata beforehand and we have a standardized table to parse. Before that, people were trying to write complicated Python scripts without and coming up with conflicting results. Now our biggest dissagrement is whether to use `grep` or `awk` for filtering.


---


.left[
###- *`head`* to truncate output for prototyping
###- *`column -t`* to pretty-print a table
###- *`grep`* and *`awk`* to filter rows
###- *`wc -l`* to count (filtered) rows
###- *`sort`* to sort output
###- *`uniq`* to filter out duplicate rows
###- *`join`* to join two sorted files on a column
]


???

So to summarize, I showed you the benefit of making some simple scripts to output tables and then I showed you how to use them with these wonderful unix tools.

...

And there are so many more that I didn't discuss.

That's all the time that I have unfortunately,


Check out the repo link below if you want these slides or to play around with these scripts, and thanks for listening!


---


## Testing

.left[
- Move as much code into your models and unit test there
- Script should produce parseable output; mock its data and parse it
- Integration test against real data (answer a Q that should remain stable)
]


---


## Performance

.left[
- cProfile
- generators for head
- pandas for full output
- use options to enable/disable rendering expensive cols
- cache output (e.g. joblib.memory)
]


---


## References

.left[
- `https://www.slideshare.net/shopifyInvestors/investor-deck-q3-2017-81481241`
- `https://www.slideshare.net/SolmazShahalizadeh/building-a-financial-data-warehouse-a-lesson-in-empathy`
- `https://brandonwamboldt.ca/how-linux-pipes-work-under-the-hood-1518/`
]

## Thanks

Karl Taylor, Aaron Olsen, and the<br>Data Acceleration team @ Shopify


    </textarea>
    <link rel="stylesheet" href="tomorrow-night-blue.css">
    <script src="highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="remark.min.js"></script>

    <script src="./mermaid.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    
    <script>
      var slideshow = remark.create({
        // Set the slideshow display ratio
        // Default: '4:3'
        // Alternatives: '16:9', ...
        ratio: '4:3',

      highlightLanguage: 'python',
      highlightStyle: 'tomorrow-night-blue',
      highlightLines: true,
      
        // Navigation options
        navigation: {
          // Enable or disable navigating using scroll
          // Default: true
          // Alternatives: false
          scroll: true,

          // Enable or disable navigation using touch
          // Default: true
          // Alternatives: false
          touch: true,

          // Enable or disable navigation using click
          // Default: false
          // Alternatives: true
          click: false
        },

        // Customize slide number label, either using a format string..
        slideNumberFormat: 'Slide %current% of %total%',
        // .. or by using a format function
        slideNumberFormat: function (current, total) {
          return current + ' of ' + total;
        },

        // Enable or disable counting of incremental slides in the slide counting
        countIncrementalSlides: true
      }); 
    </script>
  </body>
</html>
